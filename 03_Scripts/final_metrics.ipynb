{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import yaml\n",
    "import logging, argparse\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import fct_misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as fp:\n",
    "    cfg = yaml.load(fp, Loader=yaml.FullLoader)['final_metrics.py']    #  [os.path.basename(__file__)]\n",
    "\n",
    "\n",
    "# Define constants ------------------------------------\n",
    "\n",
    "DEBUG_MODE=cfg['debug_mode']\n",
    "THRESHOLD=cfg['threshold']\n",
    "CLASSES=['artificial', 'natural']\n",
    "\n",
    "PROCESSED_FOLDER=cfg['processed_folder']\n",
    "FINAL_FOLDER=cfg['final_folder']\n",
    "OD_FOLDER_100=os.path.join(PROCESSED_FOLDER, cfg['object_detector_folder_100'])\n",
    "OD_FOLDER_200=os.path.join(PROCESSED_FOLDER, cfg['object_detector_folder_200'])\n",
    "\n",
    "GROUND_TRUTH_100=cfg['input']['ground_truth_100']\n",
    "GROUND_TRUTH_200=cfg['input']['ground_truth_200']\n",
    "PREDICTIONS=cfg['input']['to_evaluate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing files...')\n",
    "\n",
    "ground_truth_100=gpd.read_file(os.path.join(PROCESSED_FOLDER, GROUND_TRUTH_100))\n",
    "ground_truth_200=gpd.read_file(os.path.join(PROCESSED_FOLDER, GROUND_TRUTH_200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_100=gpd.GeoDataFrame()\n",
    "for dataset_name in PREDICTIONS.values():\n",
    "    dataset=gpd.read_file(os.path.join(OD_FOLDER_100, dataset_name))\n",
    "    predictions_100=pd.concat([predictions_100, dataset], ignore_index=True)\n",
    "\n",
    "predictions_200=gpd.GeoDataFrame()\n",
    "for dataset_name in PREDICTIONS.values():\n",
    "    dataset=gpd.read_file(os.path.join(OD_FOLDER_200, dataset_name))\n",
    "    predictions_200=pd.concat([predictions_200, dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Formatting the data...')\n",
    "\n",
    "fct_misc.test_crs(ground_truth_100.crs, ground_truth_200.crs)\n",
    "ground_truth_100['CATEGORY']=\"artificial\"\n",
    "ground_truth_200['CATEGORY']=\"natural\"\n",
    "ground_truth=pd.concat([ground_truth_100, ground_truth_200], ignore_index=True)\n",
    "ground_truth['SUPERCATEGORY']=\"road\"\n",
    "\n",
    "predictions_100['CATEGORY']=\"artificial\"\n",
    "predictions_200['CATEGORY']=\"natural\"\n",
    "predictions=pd.concat([predictions_100, predictions_200], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the intersecting area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Getting the intersecting area...')\n",
    "\n",
    "ground_truth_2056=ground_truth.to_crs(epsg=2056)\n",
    "ground_truth_2056['area_label']=ground_truth_2056.area\n",
    "\n",
    "predictions_2056=predictions.to_crs(epsg=2056)\n",
    "predictions_2056['area_predictions']=predictions_2056.area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct_misc.test_crs(ground_truth_2056.crs, predictions_2056.crs)\n",
    "predicted_roads_2056=gpd.overlay(ground_truth_2056, predictions_2056, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_roads_filtered=predicted_roads_2056[(~predicted_roads_2056['OBJECTID'].isna()) & (~predicted_roads_2056['score'].isna())].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_roads_filtered['joined_area']=predicted_roads_filtered.area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_roads_filtered['area_pred_in_label']=round(predicted_roads_filtered['joined_area']/predicted_roads_filtered['area_label']*100, 2)\n",
    "predicted_roads_filtered['weighted_score']=predicted_roads_filtered['area_pred_in_label']*predicted_roads_filtered['score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the final type (score weighted with intersecting area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Caclulating the indexes...')\n",
    "\n",
    "final_type={'road_id':[], 'road_type':[]}\n",
    "detected_roads_id=predicted_roads_filtered['OBJECTID'].unique().tolist()\n",
    "\n",
    "for road_id in ground_truth['OBJECTID'].unique().tolist():\n",
    "\n",
    "    if road_id not in detected_roads_id:\n",
    "        final_type['road_id'].append(road_id)\n",
    "        final_type['road_type'].append('undetermined')\n",
    "        continue\n",
    "\n",
    "    intersecting_predictions=predicted_roads_filtered[predicted_roads_filtered['OBJECTID']==road_id].copy()\n",
    "\n",
    "    groups=intersecting_predictions.groupby(['CATEGORY_2']).sum()\n",
    "    if 'natural' in groups.index:\n",
    "        natural_index=groups.loc['natural', 'weighted_score']/groups.loc['natural', 'score']\n",
    "    else:\n",
    "        natural_index=0\n",
    "    if 'artificial' in groups.index:\n",
    "        artificial_index=groups.loc['artificial', 'weighted_score']/groups.loc['artificial', 'score']\n",
    "    else:\n",
    "        artificial_index=0\n",
    "\n",
    "    if artificial_index > natural_index:\n",
    "        final_type['road_id'].append(road_id)\n",
    "        final_type['road_type'].append('artificial')\n",
    "    elif artificial_index < natural_index:\n",
    "        final_type['road_id'].append(road_id)\n",
    "        final_type['road_type'].append('natural')\n",
    "    else:\n",
    "        final_type['road_id'].append(road_id)\n",
    "        final_type['road_type'].append('undetermined')\n",
    "\n",
    "final_type_df=pd.DataFrame(final_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df=final_type_df.merge(ground_truth[['OBJECTID', 'CATEGORY']], how='inner',\n",
    "                                left_on='road_id', right_on='OBJECTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    comparison_df.shape[0]==ground_truth.shape[0], \"There are to many or not enough labels in the final results\"\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating the metrics...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- Calculating the accuracy...')\n",
    "\n",
    "per_right_roads=round(comparison_df[comparison_df['CATEGORY']==comparison_df['road_type']].shape[0]/comparison_df.shape[0]*100,2)\n",
    "per_missing_roads=round(comparison_df[comparison_df['road_type']=='undetermined'].shape[0]/comparison_df.shape[0]*100,2)\n",
    "per_wrong_roads=100-per_right_roads-per_missing_roads\n",
    "\n",
    "print(f\"{per_right_roads}% of the roads were found and have the correct road type.\")\n",
    "\n",
    "print(f\"{per_missing_roads}% of the roads were not found.\")\n",
    "print(f\"{per_wrong_roads}% of the roads had the wrong road type.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- Calculating the macro balanced weighted accuracy...')\n",
    "\n",
    "metrics_dict={'class':[], 'TP':[], 'FP':[], 'FN':[], 'Pk':[], 'Rk':[], 'count':[]}\n",
    "for road_type in CLASSES:\n",
    "    metrics_dict['class'].append(road_type)\n",
    "    tp=comparison_df[(comparison_df['CATEGORY']==comparison_df['road_type']) &\n",
    "                    (comparison_df['CATEGORY']==road_type)].shape[0]\n",
    "    fp=comparison_df[(comparison_df['CATEGORY']!=comparison_df['road_type']) &\n",
    "                    (comparison_df['road_type']==road_type)].shape[0]\n",
    "    fn=comparison_df[(comparison_df['CATEGORY']!=comparison_df['road_type']) &\n",
    "                    (comparison_df['CATEGORY']==road_type)].shape[0]\n",
    "\n",
    "    metrics_dict['TP'].append(tp)\n",
    "    metrics_dict['FP'].append(fp)\n",
    "    metrics_dict['FN'].append(fn)\n",
    "\n",
    "    pk=tp/(tp+fp)\n",
    "    rk=tp/(tp+fn)\n",
    "    metrics_dict['Pk'].append(pk)\n",
    "    metrics_dict['Rk'].append(rk)\n",
    "\n",
    "    metrics_dict['count'].append(comparison_df[comparison_df['CATEGORY']==road_type].shape[0])\n",
    "\n",
    "    print(f\"The {road_type} roads have a precision of {round(pk, 2)} and a recall of {round(rk, 2)}\")\n",
    "\n",
    "metrics_df=pd.DataFrame(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pixels=metrics_df['count'].sum()\n",
    "\n",
    "precision=(metrics_df['Pk']*metrics_df['count']).sum()/total_pixels\n",
    "recall=(metrics_df['Rk']*metrics_df['count']).sum()/total_pixels\n",
    "\n",
    "f1_score=round(2*precision*recall/(precision + recall), 2)\n",
    "\n",
    "print(f\"The final F1-score for a threshold of {THRESHOLD} is {f1_score}\", \n",
    "    f\" with a precision of {round(precision,2)} and a recall of {round(recall,2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('road_surfaces')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d832f3481d940cc3bb935f7c66bfbdcf68066e3553491dd3c1b68c49468b7cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
